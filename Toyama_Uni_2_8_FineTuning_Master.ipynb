{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVhR3XHXE9xK"
      },
      "outputs": [],
      "source": [
        "!rm -rf ~/.cache/huggingface/datasets\n",
        "!rm -rf ~/.cache/huggingface/hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up_ZrRA77LYM",
        "outputId": "31cb63f3-13e5-40b1-bf3e-8dd188bcaa41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8cZUpa7lnB0",
        "outputId": "aadb6352-1179-429b-9861-07139b3a3a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# LLMファインチューニングに必要なライブラリ群\n",
        "!pip install -q \\\n",
        "  transformers \\\n",
        "  datasets \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  peft \\\n",
        "  sentencepiece \\\n",
        "  scipy \\\n",
        "  evaluate \\\n",
        "  huggingface-hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoLMGBnCD4pJ",
        "outputId": "cac25e53-91a6-4eca-dc00-70d232afbd5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (2025.3.0)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2025.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f94c7ce",
        "outputId": "59476a03-1caa-46d4-844e-27b6e110e1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fsspec==2025.3.2\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.9.0\n",
            "    Uninstalling fsspec-2025.9.0:\n",
            "      Successfully uninstalled fsspec-2025.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.3.2 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2025.3.2\n"
          ]
        }
      ],
      "source": [
        "# Downgrade fsspec to resolve the conflict\n",
        "!pip install fsspec==2025.3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3fe429298d3947a0a36fe0ddc20b9c93",
            "1dae1782b9fd48b8913f9e5a981b9e29",
            "6a1a2b6d09f34c0db9ed9519c39ddce2",
            "baad313df0b24106bfa1c00f33d88b4a",
            "7f76ad5279c54d0b8310ef9f1d18a16e",
            "dec45e567f364090be89b4ebc01a6417",
            "0119d941cd1f4f038e346a90989baf4e",
            "c6a5dcb8237f4ebf856ab7af37a807e6",
            "01c326612a9945b38b3a8ce453947566",
            "02325ea04c434d8ebcd1d2f24d8d05f9",
            "a0e43228cc8445c495c9a719f368cc91",
            "1d793917982142e2829361fb00a8343f",
            "c2e13a70e9ab49c4b8dcce3687763312",
            "0ae84e271cbb448fb1fe578ef67bbf5b",
            "cc19d1135ede47f9b6070ddc373ccf87",
            "fdbe8699ffb74202931f0027e56e159a",
            "f82f7e6fc5214a809e3773f982820c58",
            "dbd32a435bca46d19f9cdabb4fd402bb",
            "75386f9ad4564f2294fe3783eec2cb71",
            "33fc83194d2c4e51a8186588b2b1dc91",
            "09a833d28774498f93e0f22eb4ad1b55",
            "73620feb078c423eb2690c8a80fd59c9",
            "97705403f415495eb673fcfef1d3e0b3",
            "ea058a0220db46a591a54d193b2be437",
            "d03e0cb97c2e4c03ad404b8677ee1b9a",
            "10872c889aef4bd491d84bf323cea3d9",
            "5e506de27c0746e282f4f631b330389c",
            "9aebbc0cb1b84d0bb84bf5398fdb52f4",
            "2a6da628d3174c67a0bd85ce24265e5f",
            "aa5f7eb482484a44b387467d66001ae0",
            "1bb8bf619d9b4d1e8820fb98c8ac5e2d",
            "e14531aaadd44923907965761a8a293f",
            "7271fc53e06742999ede554d38e24db0",
            "6140fc98bf264577a3bcffd63eae6ab5",
            "6ba2929ed1aa49d08323fe8f224a1930",
            "556a5e1e761a434cb481a77222ee45a1",
            "71f5d87413414193801a0e2fbad4e8bf",
            "a914c682676744289eee5b870b03b1ed",
            "a2df52f3b52448d3900586f05838b3d9",
            "e095f00c3fad4192a0a01bdd4f610983",
            "235eb0f978ba4b6394bea32845f7e645",
            "cf691ed2c7284ed591b58c627e0d6128",
            "8ea7188c81234f239fc918edae833e91",
            "c0a7f75d275a45be8827a272e91ec584",
            "d3d7d5ad58414ec9ba437d51af95fae5",
            "3f60e505724a416bba5a369b913e69e0",
            "4040f27a2a9542dfaf1b3f406857f92a",
            "0f9adf9dc381449090743e4dd2b89cc2",
            "51341d5c3b114d4cbd235c084bd34dc9",
            "043fc9e68c894d239c90e26f371c3049",
            "18742c7e28104517a58ed7a02e243ac3",
            "cd193cc82321422c84fca460c0714927",
            "64b69f5045e94c3e967dfa710604d8e5",
            "444655f9b75d406092e22c75a2df2373",
            "7e6a4bbe95a546a5b5263325a4648be3",
            "43ef6825dc2f4904bdab378c42727733",
            "f19840ceba164a68be3e6b4ee1baef73",
            "7ed0e1b044c947fd97623f13cad6566a",
            "d46bc54a82494800a67f0d2aca7da9e2",
            "eb37b656858749ec841f84bf9a97f274",
            "d5d4ddd960b8440fbbe6347298131407",
            "a5de626bd8a542ac8df0ad6ad3b4c7b2",
            "f464f4b0b06c4c3b81aff74d64b37616",
            "47bb01049a9e48f0a3b6223b8daeed56",
            "c165b36e93a940c2966e5d4ae5c6131a",
            "1ae1fc1574c94aaa999b08fef0049efc",
            "945b482c620145388b0e3091f7033904",
            "25b86e36c9b240cda83fce81996ace7e",
            "0c279fa425a443398cbf42a014667e98",
            "062d13230cd048f099f6dbd5385c70eb",
            "8d8702bcc93247df975933d6f30aacd2",
            "a8d49bbb3e4b4cb1ac1db56e8b05b7e8",
            "dd9d11e6223c40f9bb229742cbc01171",
            "4d93e17c773948aba916ba0c14e5e404",
            "243d14fe717c4b28a5a2ab7a576cd9c1",
            "1319d7971d694d80aec183745b2a6505",
            "5f114e0c15c343218383e0be95b7f5e9",
            "87098504ba3e46f2b399deae14356ef5",
            "3f60657625cb425096cc484e5ff9758e",
            "3231ae74cd594d908bafa062c784d8d8",
            "a474afdce6df4a149eec538552d8063d",
            "a3454b137c374a248f81eb82bcef2105",
            "89c5f4c585fc409994df0ef8e85a656c",
            "fba82e22bc554c35a9db9c85f69f883b",
            "91e4fec97769408f824f67b5d20ded28",
            "7e6bc15743d245f8adf710b19872d440",
            "8e51ed2890f54bfc9e6ff8368ea43abb",
            "50752e388ddd4ab5a20efcf2b20d4672",
            "44fe7d384cb7403ba2ffd25e60de8a47",
            "1c94169ea45f4e15a991edeb09f0acfb",
            "4bd83530dbf64790b78ce24862cef594",
            "d5cf9ea04ea048f7a33a8c5535a816ec",
            "14758f579b3a4653a3bc2eb28918abb8",
            "223b916453f946f280095c42d2cd7268",
            "8e9c7268317045798ffd1a983f7565cc",
            "e4c4649764224623ab7ad08365f61334",
            "0636872d38e64dbeb9a400a2f7904293",
            "f239b9faa9d54a6f88d309418f39f5d5",
            "ad3ccf180f754bbf9c028bf70d2940ee",
            "4a75ff317beb4a2486e7c08a8be70c8f",
            "c6176eb2105f4572b399f163369c9da5",
            "6a370abfa64d4484b9bcab6fcfd64d68",
            "dbe77c3443e749b1b151bfb154dbf83c",
            "3bde9df232d9499da2f23aeb3488c8b3",
            "6407f571368c40e48f2d46e0eb9cf20a",
            "a8d6e0eaa01343c8971b17f3d1c72cf6",
            "76ba00e32f7c473e8817bf4ec0804517",
            "404540a141a5463e81e611b2366ab237",
            "bed642d86dc64cce9f7f2ec0bcb3ef6e",
            "fc90c44f819847dba34393400ebf8cba",
            "0f08b53cd0bc486b9508a566a9f9a22c",
            "f334a18417d54679814c73a620bbf437",
            "1aead960169d4320a83ec9c9d5d45418",
            "9fe2b272fdb448c7a1f287c6c0bec3c4",
            "cf7fb2fb03d84617aaa03290b6850131",
            "052079709c6844e98edb23280eaaf7e3",
            "7244b742b1314cf1b9ec2106a32438ec",
            "3c716dbbce6647108987e81cb23d13c3",
            "c4965700eed2446a996c9e5e47d542cb",
            "b07cc09579444ab5ab81014526f0bf29",
            "2e32ec2a58624639be93492cf337f0b4",
            "d8bbdd333ef04858a574becf34ea5d1f",
            "b3fac7bc7cfa47fab16d12bebc8f61cd",
            "904391927d854fceb2014cc41a43013c",
            "3d02c1e32b4d4a679ee0482d7eae8309",
            "4c34df217d7a4d1089660793ad91793f",
            "e5bf8ccd5c9b4cdfb800c775209024ac",
            "2a9dd824492f45fdaa5eb79fe73e69de",
            "ffed0b03ee9d4427a542f10e9ce97765",
            "0c5950d89793458cb2a55b2f5e839b6b",
            "666a0aaa40e04e55a93125e4cd2b55d5",
            "dc02240152ef4cef8fe80baa007409be",
            "8d0261e16dac455e9b0fb8c6cf95a879",
            "8a8424d687fe4b6ca69c018d1b283c83",
            "c4662a049d9f4aeb925f50698a77f117",
            "96d7b630d8414ab0af28edc5ffdce015",
            "775a1a773cdc48c9964c615358f6aea7",
            "b17112a759c44b45ba7508752705cbdf",
            "8ac3bc71da684a778cae2c0911481c97",
            "96122f69732c4d048c67dd984e00710e",
            "a4ce5790035b40369613975c0ae5b7a4",
            "bd173a788fa24383aea5927eb5dcfef8",
            "f0cff92d9a1e4e858e138c979725079b",
            "e3543f5ce6064d75a7edb30b4c8cf598",
            "f138726a7e5043c58ca68f51cc87c8f1",
            "c798e8854f92473ba7db9d73ddf9fe21",
            "5b92db50347f4ee38c5c7c19d2df7706",
            "5b401cc3fc1740369a0e69220203905d",
            "ee57fe1889b74a57bfdc955d880ed7b5",
            "0809db03d5874a88b24e59bff70f508f",
            "6d833443b772460ca6e61edc980d4cae",
            "637ee341ac574d3d917e80bccf38725d",
            "95c0a12528c94557aef68b69623162c7",
            "60c5f15d9bc649ada736505f43b5d72f",
            "1be7ac9b645645c2a3c5e3b752377499",
            "5a3605fa8a664d44bc8d6722c330c39c",
            "f3eb0d424f2c447ea4c27f4543c2b4f2",
            "320695e912e2451583bbff0d5871db5f",
            "d0c45063e8f74efba795534f5f2c8eea",
            "85dfbd452c894e5d81b2207d1f3f73a2",
            "20c7d1d3ecfc4778ae96923719074eb8",
            "1a692fb02c5e4be7af95cbbf0a1fbd02",
            "40d96ffa79ed4c0c8116ce10b95d9250",
            "b712c0f3ec4941c6bea4797360698060",
            "d26256386ac443f9bf8b8d1b9d90ad40",
            "7b486edd28b846488479b12f28f66f67",
            "2b33ff1a0efa4c8081d6853acb88b4be",
            "6fee5b1f4c24460ea4465eae81f4a0f6",
            "ea37684082684a88b2433932602af6e7",
            "12bf9c25c5ad47bda08d0a5e21d1f15c",
            "469921de1df84e49963acbee660af509",
            "04b7ee6a607349e991b8de1214cea425",
            "6ea9f3e7877348538131a3bfec244170",
            "abd8cf60b0d34f8e9da7a16b04532bf9",
            "8f6cddb97344441580f9dd4ee46d0522",
            "40b124d9f83b4317bf32644f2acddd92",
            "47c9c4c59e2d47548cc517fdf36fe8bf",
            "60282242983f4731847352b1b991695e",
            "761dfe3e5b434b1b8e6b3010dbae1cc9",
            "14923f6fc26046fca5e6c9cd7fe84e29",
            "35ea8094e5dd4ef085440df04309229a",
            "b1805b83c98f4ed99ebc6746f9254d79",
            "8ae0799ff7a8407c91b425679c832bc8",
            "a1d57b15cfa04bff949a2ed614a24fcd",
            "73311ecc15cb4d1f88915efe6ca1d139",
            "1fda01ba803e45d0b70313b6e8d0552f",
            "d19e2de5f5d1491780b0bf69ad44c39d",
            "da838c912a514d8f92a4b8df8105ed06",
            "30bf0f87f8cf4715872eeb75f1c94428",
            "f784303d9faa4f299db8e7c69a266664",
            "f0617d5701c34197921bad7609e7b6b1",
            "a9eef9558ade4eecb132d8c63d7f3fef",
            "0e856cdf619e4b3d9fa81037bf86da7c",
            "36464882888c48529c02f795228ca231",
            "ef3a4952bea04059bda92ce9f3d4f362",
            "ceeaa555b1ab47ec81ce87251d5987b5",
            "0d94ae8ee7634d518f77043fdc15c3ef",
            "c449790dfc464a408e2d5f4798f4a09c"
          ]
        },
        "id": "pRJZE5tVlIg8",
        "outputId": "3ebc5b37-00b4-4adb-8784-075787423245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fe429298d3947a0a36fe0ddc20b9c93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d793917982142e2829361fb00a8343f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97705403f415495eb673fcfef1d3e0b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6140fc98bf264577a3bcffd63eae6ab5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3d7d5ad58414ec9ba437d51af95fae5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43ef6825dc2f4904bdab378c42727733",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "945b482c620145388b0e3091f7033904",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87098504ba3e46f2b399deae14356ef5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44fe7d384cb7403ba2ffd25e60de8a47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a75ff317beb4a2486e7c08a8be70c8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f08b53cd0bc486b9508a566a9f9a22c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8bbdd333ef04858a574becf34ea5d1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d0261e16dac455e9b0fb8c6cf95a879",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3543f5ce6064d75a7edb30b4c8cf598",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1be7ac9b645645c2a3c5e3b752377499",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b486edd28b846488479b12f28f66f67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47c9c4c59e2d47548cc517fdf36fe8bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da838c912a514d8f92a4b8df8105ed06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 02:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.555300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.177200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.107900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.097800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.948000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.037000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.053500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.968200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.110500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.008100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.869800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.927100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=125, training_loss=2.0633645706176758, metrics={'train_runtime': 136.9148, 'train_samples_per_second': 7.304, 'train_steps_per_second': 0.913, 'total_flos': 5089791049728000.0, 'train_loss': 2.0633645706176758, 'epoch': 1.0})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from huggingface_hub import login\n",
        "\n",
        "# デバイス確認（T4 GPU が見えるか）\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on {device}\")\n",
        "\n",
        "# 1. モデルとトークナイザの準備\n",
        "# model_name = \"rinna/japanese-gpt2-small\"\n",
        "model_name = \"elyza/ELYZA-japanese-Llama-2-7b\"\n",
        "\n",
        "# 8bit量子化の設定\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_has_fp16_weight=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16  # T4 GPU に最適\n",
        ")\n",
        "\n",
        "# # Instead, move the model to the device explicitly after loading.\n",
        "# model.to(device)\n",
        "\n",
        "# # 2. LoRAの設定 GPT2系のrinna/japanese-gpt2-smallの場合（PEFT）\n",
        "# lora_config = LoraConfig(\n",
        "#     r=8,\n",
        "#     lora_alpha=32,\n",
        "#     target_modules=[\"c_proj\", \"c_attn\"],  # Changed target modules to match GPT2 architecture\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=TaskType.CAUSAL_LM\n",
        "# )\n",
        "\n",
        "# 2. LoRAの設定 Llama2系 ELYZAの場合\n",
        "  # モジュール名\t用途・場所\n",
        "  # q_proj\tAttentionのQuery部分\n",
        "  # k_proj\tAttentionのKey部分\n",
        "  # v_proj\tAttentionのValue部分\n",
        "  # o_proj\tAttentionの出力変換部分\n",
        "  # gate_proj\tMLP(FFN)部のGating層\n",
        "  # up_proj\tMLP(FFN)部の中間変換層\n",
        "  # down_proj\tMLP(FFN)部の出力層\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"      # MLP\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# 3. データセット読み込みと前処理 スプリット指定は使わない（問題の回避）\n",
        "# dataset = load_dataset(\"ag_news\", split=\"train\")\n",
        "dataset = load_dataset(\"izumi-lab/llm-japanese-dataset\", split=\"train\")\n",
        "dataset_small = dataset.select(range(1000))\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset_small.map(tokenize_fn, batched=True)\n",
        "\n",
        "\n",
        "# 4. トレーニング引数の設定\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1, #本当は3回ぐらい回したい\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# 5. トレーナー定義と学習開始\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCcvffwb03A4"
      },
      "source": [
        "# Fine-Tuning後のモデルを利用して推論(Chat)を実行\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgkO4WOVlnAO",
        "outputId": "5c98128f-a704-40c0-f8b8-df86741b072f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 回答: （106）、「国内景気が回復する見通しで上昇」（58）、「金利上昇により企業の資金調達コストが高まる」（43）、「景気後退の影響から下落」（37） 自民党の石破茂政調会長は29日午前の記者会見で、安\n"
          ]
        }
      ],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# より構造化されたプロンプト（LoRAで学習している形式に合わせること）\n",
        "prompt = \"このニュースを分類してください：「株式市場が好調な企業業績を受けて上昇」\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# 推論設定\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7,\n",
        "    repetition_penalty=1.1,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    bos_token_id=tokenizer.bos_token_id\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        generation_config=generation_config\n",
        "    )\n",
        "\n",
        "# 出力をプロンプトと分離して表示\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"回答:\", generated_text.replace(prompt, \"\").strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_zBMAzQ7ri5"
      },
      "source": [
        "#日本語データセット版"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
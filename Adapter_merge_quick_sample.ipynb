{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0YUFfYg2y/l4rY//bkeef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takedatmh/toyama/blob/main/Adapter_merge_quick_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx5yf1vXkVaA",
        "outputId": "ada56e53-b08e-4f88-8cfe-1447cd076cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRAãƒãƒ¼ã‚¸ - å³åº§ã«å®Ÿè¡Œå¯èƒ½ãªã‚³ãƒ¼ãƒ‰\n",
            "==================================================\n",
            "\n",
            "### ä¾‹1: OPT-125Mã§ã®åŸºæœ¬ãƒãƒ¼ã‚¸ï¼ˆCPUå¯ã€ãƒ¡ãƒ¢ãƒª500MBç¨‹åº¦ï¼‰\n",
            "ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\n",
            "LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 147,456\n",
            "\n",
            "ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒãƒ¼ã‚¸ä¸­...\n",
            "\n",
            "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: 'AI technology is'\n",
            "ç”Ÿæˆä¸­...\n",
            "çµæœ: AI technology is a \"thought-driven\" process. We live with artificial intelligence, and we live with it. Today, the tech giant IBM\n",
            "\n",
            "\n",
            "### ä¾‹2: GPT-2ã§ã®ãƒãƒ¼ã‚¸ä¾‹ï¼ˆCPUå¯ã€ãƒ¡ãƒ¢ãƒª1.5GBç¨‹åº¦ï¼‰\n",
            "GPT-2ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\n",
            "\n",
            "æˆ¦ç•¥: tech_creative_mix\n",
            "  å…¥åŠ›: The future of artificial intelligence\n",
            "  å‡ºåŠ›: The future of artificial intelligence is unclear, but the company is set to unveil a new platform to take advantage of the high-tech capabilities of the next generation of AI.\n",
            "\n",
            "The new platform,\n",
            "\n",
            "æˆ¦ç•¥: all_balanced\n",
            "  å…¥åŠ›: The future of artificial intelligence\n",
            "  å‡ºåŠ›: The future of artificial intelligence and the future of the world is in flux.\n",
            "\n",
            "This post originally appeared on Digital Trends.\n",
            "\n",
            "We are excited to announce that Samsung has finally created a new smart\n",
            "\n",
            "\n",
            "### ä¾‹3: ãƒãƒ¼ã‚¸æ‰‹æ³•ã®æ¯”è¼ƒ\n",
            "\n",
            "ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: 'Machine learning is'\n",
            "\n",
            "å„ãƒãƒ¼ã‚¸æ‰‹æ³•ã®çµæœ:\n",
            "\n",
            "[linear]: Machine learning is the most efficient way to learn, and it's going to do a lot of good for you.\n",
            "\n",
            "\n",
            "[cat]: Machine learning is a fundamental part of the human mind. It is often referred to as machine learning.\n",
            "\n",
            "The human\n",
            "\n",
            "[ties]: Machine learning is the next big thing but itâ€™s going to take a while to get here. Itâ€™\n",
            "\n",
            "[dare_linear]: Machine learning is a new approach to solving complex problems, from the engineering perspective, and as such is expected to be a\n",
            "\n",
            "\n",
            "### ä¾‹4: å®Ÿç”¨çš„ãªãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®ä½¿ç”¨\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®çµæœ:\n",
            "The secret to happiness is to be happy.\n",
            "I'm not sure if you're being sarcastic or not, but I think you're right.\n",
            "\n",
            "==================================================\n",
            "âœ… ã™ã¹ã¦ã®ä¾‹ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n",
            "\n",
            "å®Ÿè¡Œç’°å¢ƒ:\n",
            "- PyTorch ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 2.6.0+cu124\n",
            "- ãƒ‡ãƒã‚¤ã‚¹: CUDA\n",
            "- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: ã™ã¹ã¦ã®ä¾‹ãŒCPUã§å®Ÿè¡Œå¯èƒ½\n",
            "\n",
            "ğŸ’¡ ãƒ’ãƒ³ãƒˆ:\n",
            "1. ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™å ´åˆã¯ã€load_in_8bit=True ã‚’ä½¿ç”¨\n",
            "2. å®Ÿéš›ã®è¨“ç·´æ¸ˆã¿ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯HuggingFace Hubã§æ¤œç´¢å¯èƒ½\n",
            "3. ãƒãƒ¼ã‚¸æ‰‹æ³•ã‚’å¤‰ãˆã‚‹ã“ã¨ã§ç•°ãªã‚‹ç‰¹æ€§ãŒå¾—ã‚‰ã‚Œã¾ã™\n",
            "   - linear: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸçµåˆ\n",
            "   - cat: å®Œå…¨ãªæ©Ÿèƒ½ä¿æŒï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨å¢—ï¼‰\n",
            "   - ties: ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’è€ƒæ…®ã—ãŸçµåˆ\n",
            "   - dare_linear: ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åŠ ãˆãŸçµåˆ\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "LoRAãƒãƒ¼ã‚¸ - ä»Šã™ãå®Ÿè¡Œã§ãã‚‹æœ€å°æ§‹æˆã‚³ãƒ¼ãƒ‰\n",
        "CPUã§ã‚‚å‹•ä½œã—ã€å®Ÿéš›ã®HuggingFaceã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½¿ç”¨å¯èƒ½\n",
        "\"\"\"\n",
        "\n",
        "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆåˆå›ã®ã¿ï¼‰\n",
        "# pip install transformers peft torch --upgrade\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"LoRAãƒãƒ¼ã‚¸ - å³åº§ã«å®Ÿè¡Œå¯èƒ½ãªã‚³ãƒ¼ãƒ‰\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ===========================\n",
        "# å®Ÿè¡Œä¾‹1: æœ€è»½é‡ãƒ¢ãƒ‡ãƒ«ã§åŸºæœ¬çš„ãªãƒãƒ¼ã‚¸\n",
        "# ===========================\n",
        "\n",
        "print(\"\\n### ä¾‹1: OPT-125Mã§ã®åŸºæœ¬ãƒãƒ¼ã‚¸ï¼ˆCPUå¯ã€ãƒ¡ãƒ¢ãƒª500MBç¨‹åº¦ï¼‰\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
        "print(\"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
        "\n",
        "# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®š\n",
        "peft_config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
        "model = get_peft_model(base_model, peft_config)\n",
        "print(f\"LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.get_nb_trainable_parameters()[0]:,}\")\n",
        "\n",
        "# 2ã¤ç›®ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ \n",
        "model.add_adapter(\"adapter2\", peft_config)\n",
        "\n",
        "# ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒãƒ¼ã‚¸å®Ÿè¡Œ\n",
        "print(\"\\nã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒãƒ¼ã‚¸ä¸­...\")\n",
        "model.add_weighted_adapter(\n",
        "    adapters=[\"default\", \"adapter2\"],\n",
        "    weights=[0.6, 0.4],\n",
        "    adapter_name=\"merged\",\n",
        "    combination_type=\"linear\"\n",
        ")\n",
        "\n",
        "# ãƒãƒ¼ã‚¸ã—ãŸã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã§ç”Ÿæˆ\n",
        "model.set_adapter(\"merged\")\n",
        "prompt = \"AI technology is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"\\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'\")\n",
        "print(\"ç”Ÿæˆä¸­...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_length=30, temperature=0.8, do_sample=True)\n",
        "\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"çµæœ: {result}\")\n",
        "\n",
        "# ===========================\n",
        "# å®Ÿè¡Œä¾‹2: GPT-2ã§ã®å®Ÿç”¨çš„ãªä¾‹\n",
        "# ===========================\n",
        "\n",
        "print(\"\\n\\n### ä¾‹2: GPT-2ã§ã®ãƒãƒ¼ã‚¸ä¾‹ï¼ˆCPUå¯ã€ãƒ¡ãƒ¢ãƒª1.5GBç¨‹åº¦ï¼‰\")\n",
        "\n",
        "# GPT-2ãƒ¢ãƒ‡ãƒ«ï¼ˆæ—¥æœ¬èªã§ã‚‚è‹±èªã§ã‚‚å‹•ä½œï¼‰\n",
        "print(\"GPT-2ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "\n",
        "# GPT-2ç”¨ã®LoRAè¨­å®š\n",
        "gpt2_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å¯¾å¿œ\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# PEFT model creation with the first adapter (\"default\")\n",
        "gpt2_peft = get_peft_model(gpt2_model, gpt2_config)\n",
        "\n",
        "# Add additional adapters\n",
        "adapters = {\n",
        "    \"technical\": {\"r\": 8, \"alpha\": 32},\n",
        "    \"creative\": {\"r\": 8, \"alpha\": 16},\n",
        "    \"balanced\": {\"r\": 8, \"alpha\": 24}\n",
        "}\n",
        "\n",
        "for name, params in adapters.items():\n",
        "    config = LoraConfig(\n",
        "        r=params[\"r\"],\n",
        "        lora_alpha=params[\"alpha\"],\n",
        "        target_modules=[\"c_attn\", \"c_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )\n",
        "    # Add adapter with the specified name\n",
        "    gpt2_peft.add_adapter(name, config)\n",
        "\n",
        "\n",
        "# æ§˜ã€…ãªãƒãƒ¼ã‚¸æˆ¦ç•¥\n",
        "merge_strategies = [\n",
        "    {\n",
        "        \"name\": \"tech_creative_mix\",\n",
        "        \"adapters\": [\"default\", \"technical\", \"creative\"],\n",
        "        \"weights\": [0.2, 0.5, 0.3],\n",
        "        \"method\": \"linear\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"all_balanced\",\n",
        "        \"adapters\": [\"default\", \"technical\", \"creative\", \"balanced\"],\n",
        "        \"weights\": [0.25, 0.25, 0.25, 0.25],\n",
        "        \"method\": \"linear\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# ãƒãƒ¼ã‚¸å®Ÿè¡Œã¨ãƒ†ã‚¹ãƒˆ\n",
        "test_prompts = [\n",
        "    \"The future of artificial intelligence\",\n",
        "    \"How to write better code\",\n",
        "    \"Once upon a time in a digital world\"\n",
        "]\n",
        "\n",
        "for strategy in merge_strategies:\n",
        "    print(f\"\\næˆ¦ç•¥: {strategy['name']}\")\n",
        "\n",
        "    # ãƒãƒ¼ã‚¸å®Ÿè¡Œ\n",
        "    gpt2_peft.add_weighted_adapter(\n",
        "        adapters=strategy[\"adapters\"],\n",
        "        weights=strategy[\"weights\"],\n",
        "        adapter_name=strategy[\"name\"],\n",
        "        combination_type=strategy[\"method\"]\n",
        "    )\n",
        "\n",
        "    gpt2_peft.set_adapter(strategy[\"name\"])\n",
        "\n",
        "    # æœ€åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ†ã‚¹ãƒˆ\n",
        "    prompt = test_prompts[0]\n",
        "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = gpt2_peft.generate(\n",
        "            **inputs,\n",
        "            max_length=40,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=gpt2_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    result = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"  å…¥åŠ›: {prompt}\")\n",
        "    print(f\"  å‡ºåŠ›: {result}\")\n",
        "\n",
        "# ===========================\n",
        "# å®Ÿè¡Œä¾‹3: ç•°ãªã‚‹ãƒãƒ¼ã‚¸æ‰‹æ³•ã®æ¯”è¼ƒ\n",
        "# ===========================\n",
        "\n",
        "print(\"\\n\\n### ä¾‹3: ãƒãƒ¼ã‚¸æ‰‹æ³•ã®æ¯”è¼ƒ\")\n",
        "\n",
        "# æ–°ã—ã„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«\n",
        "comparison_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
        "comparison_peft = get_peft_model(comparison_model, peft_config)\n",
        "\n",
        "# ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ \n",
        "for i in range(2):\n",
        "    comparison_peft.add_adapter(f\"adapter_{i+1}\", peft_config)\n",
        "\n",
        "# ç•°ãªã‚‹ãƒãƒ¼ã‚¸æ‰‹æ³•ã‚’è©¦ã™\n",
        "methods_to_test = {\n",
        "    \"linear\": {},\n",
        "    \"cat\": {},  # concatenation\n",
        "    \"ties\": {\"density\": 0.5, \"majority_sign_method\": \"frequency\"},\n",
        "    \"dare_linear\": {\"density\": 0.8}\n",
        "}\n",
        "\n",
        "prompt = \"Machine learning is\"\n",
        "print(f\"\\nãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'\")\n",
        "print(\"\\nå„ãƒãƒ¼ã‚¸æ‰‹æ³•ã®çµæœ:\")\n",
        "\n",
        "for method_name, extra_params in methods_to_test.items():\n",
        "    try:\n",
        "        # ãƒãƒ¼ã‚¸å®Ÿè¡Œ\n",
        "        merge_name = f\"merged_{method_name}\"\n",
        "        comparison_peft.add_weighted_adapter(\n",
        "            adapters=[\"default\", \"adapter_1\"],\n",
        "            weights=[0.6, 0.4],\n",
        "            adapter_name=merge_name,\n",
        "            combination_type=method_name,\n",
        "            **extra_params\n",
        "        )\n",
        "\n",
        "        comparison_peft.set_adapter(merge_name)\n",
        "\n",
        "        # ç”Ÿæˆ\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = comparison_peft.generate(\n",
        "                **inputs,\n",
        "                max_length=25,\n",
        "                temperature=0.7,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"\\n[{method_name}]: {result}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[{method_name}]: ã‚¨ãƒ©ãƒ¼ - {str(e)[:50]}...\")\n",
        "\n",
        "# ===========================\n",
        "# å®Ÿè¡Œä¾‹4: å®Ÿç”¨çš„ãªãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\n",
        "# ===========================\n",
        "\n",
        "def quick_lora_merge(\n",
        "    model_name=\"facebook/opt-125m\",\n",
        "    adapter_configs=None,\n",
        "    merge_weights=None,\n",
        "    merge_method=\"linear\",\n",
        "    test_prompt=\"Hello, AI!\"\n",
        "):\n",
        "    \"\"\"\n",
        "    LoRAãƒãƒ¼ã‚¸ã‚’ç°¡å˜ã«å®Ÿè¡Œã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\n",
        "\n",
        "    Args:\n",
        "        model_name: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å\n",
        "        adapter_configs: ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®šã®ãƒªã‚¹ãƒˆ\n",
        "        merge_weights: ãƒãƒ¼ã‚¸æ™‚ã®é‡ã¿\n",
        "        merge_method: ãƒãƒ¼ã‚¸æ‰‹æ³•\n",
        "        test_prompt: ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
        "\n",
        "    Returns:\n",
        "        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ\n",
        "    \"\"\"\n",
        "    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n",
        "    if adapter_configs is None:\n",
        "        adapter_configs = [\n",
        "            {\"r\": 4, \"alpha\": 16},\n",
        "            {\"r\": 4, \"alpha\": 32}\n",
        "        ]\n",
        "\n",
        "    if merge_weights is None:\n",
        "        merge_weights = [0.5] * len(adapter_configs)\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰\n",
        "    base = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # æœ€åˆã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼\n",
        "    config = LoraConfig(\n",
        "        r=adapter_configs[0][\"r\"],\n",
        "        lora_alpha=adapter_configs[0][\"alpha\"],\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )\n",
        "\n",
        "    peft_model = get_peft_model(base, config)\n",
        "\n",
        "    # è¿½åŠ ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼\n",
        "    adapter_names = [\"default\"]\n",
        "    for i, cfg in enumerate(adapter_configs[1:], 1):\n",
        "        name = f\"adapter_{i}\"\n",
        "        adapter_names.append(name)\n",
        "        config = LoraConfig(\n",
        "            r=cfg[\"r\"],\n",
        "            lora_alpha=cfg[\"alpha\"],\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.1,\n",
        "            bias=\"none\",\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "        )\n",
        "        peft_model.add_adapter(name, config)\n",
        "\n",
        "    # ãƒãƒ¼ã‚¸\n",
        "    peft_model.add_weighted_adapter(\n",
        "        adapters=adapter_names,\n",
        "        weights=merge_weights,\n",
        "        adapter_name=\"final_merge\",\n",
        "        combination_type=merge_method\n",
        "    )\n",
        "\n",
        "    peft_model.set_adapter(\"final_merge\")\n",
        "\n",
        "    # ç”Ÿæˆ\n",
        "    inputs = tok(test_prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = peft_model.generate(**inputs, max_length=50, temperature=0.8)\n",
        "\n",
        "    return tok.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®ä½¿ç”¨ä¾‹\n",
        "print(\"\\n\\n### ä¾‹4: å®Ÿç”¨çš„ãªãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®ä½¿ç”¨\")\n",
        "\n",
        "result = quick_lora_merge(\n",
        "    adapter_configs=[\n",
        "        {\"r\": 8, \"alpha\": 16},\n",
        "        {\"r\": 8, \"alpha\": 32},\n",
        "        {\"r\": 8, \"alpha\": 24}\n",
        "    ],\n",
        "    merge_weights=[0.5, 0.3, 0.2],\n",
        "    merge_method=\"linear\",\n",
        "    test_prompt=\"The secret to happiness is\"\n",
        ")\n",
        "\n",
        "print(f\"ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®çµæœ:\\n{result}\")\n",
        "\n",
        "# ===========================\n",
        "# å®Ÿè¡Œå®Œäº†\n",
        "# ===========================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ã™ã¹ã¦ã®ä¾‹ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
        "print(\"\\nå®Ÿè¡Œç’°å¢ƒ:\")\n",
        "print(f\"- PyTorch ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}\")\n",
        "print(f\"- ãƒ‡ãƒã‚¤ã‚¹: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: ã™ã¹ã¦ã®ä¾‹ãŒCPUã§å®Ÿè¡Œå¯èƒ½\")\n",
        "\n",
        "print(\"\\nğŸ’¡ ãƒ’ãƒ³ãƒˆ:\")\n",
        "print(\"1. ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™å ´åˆã¯ã€load_in_8bit=True ã‚’ä½¿ç”¨\")\n",
        "print(\"2. å®Ÿéš›ã®è¨“ç·´æ¸ˆã¿ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯HuggingFace Hubã§æ¤œç´¢å¯èƒ½\")\n",
        "print(\"3. ãƒãƒ¼ã‚¸æ‰‹æ³•ã‚’å¤‰ãˆã‚‹ã“ã¨ã§ç•°ãªã‚‹ç‰¹æ€§ãŒå¾—ã‚‰ã‚Œã¾ã™\")\n",
        "print(\"   - linear: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸçµåˆ\")\n",
        "print(\"   - cat: å®Œå…¨ãªæ©Ÿèƒ½ä¿æŒï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨å¢—ï¼‰\")\n",
        "print(\"   - ties: ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’è€ƒæ…®ã—ãŸçµåˆ\")\n",
        "print(\"   - dare_linear: ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åŠ ãˆãŸçµåˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€**LoRAï¼ˆLow-Rank Adaptationï¼‰ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒãƒ¼ã‚¸æŠ€è¡“**ã‚’å®Ÿæ¼”ã™ã‚‹ã‚‚ã®ã§ã€è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ã¦æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®æŒ¯ã‚‹èˆã„ã‚’ä½œã‚Šå‡ºã™æ–¹æ³•ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "## ã‚³ãƒ¼ãƒ‰ã®ä¸»è¦ãªæ§‹æˆ\n",
        "\n",
        "### 1. **LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®åŸºæœ¬è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**\n",
        "\n",
        "```python\n",
        "peft_config = LoraConfig(\n",
        "    r=4,                              # ãƒ©ãƒ³ã‚¯ï¼ˆä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã®æ¬¡å…ƒæ•°ï¼‰\n",
        "    lora_alpha=16,                    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # é©ç”¨å¯¾è±¡ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
        "    lora_dropout=0.1,                 # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡\n",
        "    bias=\"none\",                      # ãƒã‚¤ã‚¢ã‚¹ã®æ‰±ã„\n",
        "    task_type=TaskType.CAUSAL_LM,    # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—\n",
        ")\n",
        "```\n",
        "\n",
        "### 2. **å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°è§£èª¬**\n",
        "\n",
        "#### **`r`ï¼ˆãƒ©ãƒ³ã‚¯ï¼‰**\n",
        "- ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—åˆ†è§£ã®æ¬¡å…ƒæ•°ã‚’æŒ‡å®š\n",
        "- å°ã•ã„å€¤ï¼ˆ4-8ï¼‰ï¼šãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã ãŒè¡¨ç¾åŠ›ã¯é™å®šçš„\n",
        "- å¤§ãã„å€¤ï¼ˆ16-32ï¼‰ï¼šã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ãŒå¯èƒ½ã ãŒãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ \n",
        "- ä¾‹ï¼š`r=4`ã¯å…ƒã®é‡ã¿è¡Œåˆ—ã‚’4æ¬¡å…ƒã®ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã§è¿‘ä¼¼\n",
        "\n",
        "#### **`lora_alpha`ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ï¼‰**\n",
        "- LoRAã®æ›´æ–°ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ä¿‚æ•°\n",
        "- å®Ÿè³ªçš„ãªå­¦ç¿’ç‡ã®ã‚ˆã†ãªå½¹å‰²ï¼š`lora_alpha / r`\n",
        "- ä¾‹ï¼š`r=4, lora_alpha=16`ã®å ´åˆã€å®ŸåŠ¹çš„ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯4å€\n",
        "\n",
        "#### **`target_modules`ï¼ˆå¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰**\n",
        "- LoRAã‚’é©ç”¨ã™ã‚‹å…·ä½“çš„ãªãƒ¬ã‚¤ãƒ¤ãƒ¼/ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
        "- ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã”ã¨ã«ç•°ãªã‚‹ï¼š\n",
        "  - OPT/LLaMA: `[\"q_proj\", \"v_proj\"]`ï¼ˆQuery/ValueæŠ•å½±ï¼‰\n",
        "  - GPT-2: `[\"c_attn\", \"c_proj\"]`ï¼ˆAttention/Projectionï¼‰\n",
        "- ã“ã‚Œã‚‰ã¯è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã®é‡è¦ãªéƒ¨åˆ†\n",
        "\n",
        "### 3. **ãƒãƒ¼ã‚¸æˆ¦ç•¥ã®å®Ÿè£…ä¾‹**\n",
        "\n",
        "```python\n",
        "# è¤‡æ•°ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é‡ã¿ä»˜ããƒãƒ¼ã‚¸\n",
        "model.add_weighted_adapter(\n",
        "    adapters=[\"default\", \"adapter2\"],  # ãƒãƒ¼ã‚¸ã™ã‚‹ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼å\n",
        "    weights=[0.6, 0.4],               # å„ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é‡ã¿\n",
        "    adapter_name=\"merged\",            # æ–°ã—ã„ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼å\n",
        "    combination_type=\"linear\"         # ãƒãƒ¼ã‚¸æ‰‹æ³•\n",
        ")\n",
        "```\n",
        "\n",
        "### 4. **ç•°ãªã‚‹ãƒãƒ¼ã‚¸æ‰‹æ³•**\n",
        "\n",
        "ã‚³ãƒ¼ãƒ‰ã§ã¯4ã¤ã®ãƒãƒ¼ã‚¸æ‰‹æ³•ã‚’æ¯”è¼ƒï¼š\n",
        "\n",
        "- **`linear`**: å˜ç´”ãªç·šå½¢çµåˆï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰\n",
        "- **`cat`**: é€£çµï¼ˆå…¨æ©Ÿèƒ½ä¿æŒã€ãƒ¡ãƒ¢ãƒªå¢—ï¼‰\n",
        "- **`ties`**: ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§è€ƒæ…®ï¼ˆå¯†åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ãï¼‰\n",
        "- **`dare_linear`**: ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åŠ ãˆãŸç·šå½¢çµåˆ\n",
        "\n",
        "### 5. **å®Ÿç”¨çš„ãªä½¿ç”¨ä¾‹**\n",
        "\n",
        "```python\n",
        "# GPT-2ã§ã®è¤‡æ•°ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®š\n",
        "adapters = {\n",
        "    \"technical\": {\"r\": 8, \"alpha\": 32},  # æŠ€è¡“çš„ãªæ–‡ç« ç”¨\n",
        "    \"creative\": {\"r\": 8, \"alpha\": 16},   # å‰µé€ çš„ãªæ–‡ç« ç”¨\n",
        "    \"balanced\": {\"r\": 8, \"alpha\": 24}    # ãƒãƒ©ãƒ³ã‚¹å‹\n",
        "}\n",
        "```\n",
        "\n",
        "å„ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯ç•°ãªã‚‹`alpha`å€¤ã‚’æŒã¡ã€ç•°ãªã‚‹ã€Œæ€§æ ¼ã€ã‚’è¡¨ç¾ï¼š\n",
        "- **technical**: é«˜ã„alphaï¼ˆ32ï¼‰ã§å¼·ã„å½±éŸ¿\n",
        "- **creative**: ä½ã„alphaï¼ˆ16ï¼‰ã§æŸ”è»Ÿãªè¡¨ç¾\n",
        "- **balanced**: ä¸­é–“çš„ãªå€¤ï¼ˆ24ï¼‰\n",
        "\n",
        "### 6. **ãƒãƒ¼ã‚¸ã®å®Ÿè¡Œãƒ•ãƒ­ãƒ¼**\n",
        "\n",
        "1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "2. åˆæœŸLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½œæˆ\n",
        "3. è¿½åŠ ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½œæˆï¼ˆç•°ãªã‚‹è¨­å®šã§ï¼‰\n",
        "4. é‡ã¿ä»˜ããƒãƒ¼ã‚¸ã‚’å®Ÿè¡Œ\n",
        "5. ãƒãƒ¼ã‚¸ã—ãŸã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
        "\n",
        "ã“ã®ã‚³ãƒ¼ãƒ‰ã®å„ªã‚ŒãŸç‚¹ã¯ã€**CPUç’°å¢ƒã§ã‚‚å‹•ä½œ**ã—ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ã‚‰ã‚Œã‚‹ã“ã¨ã§ã™ã€‚å®Ÿéš›ã®ç”¨é€”ã§ã¯ã€ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ã§è¨“ç·´ã•ã‚ŒãŸã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒãƒ¼ã‚¸ã—ã¦ã€å¤šæ©Ÿèƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã§ãã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "LImJCMNItSts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UJrxEPQAtZdE"
      }
    }
  ]
}